--- app.py2023-06-15 14:30:45
+++ app.py.new2023-06-15 14:30:45
@@ -3460,7 +3460,7 @@
         try:
             # Try to use the enhanced formatter if available
             from web.multi_model_processor import format_enhanced_prompt
-            formatted_prompt = format_enhanced_prompt(message, model_type="zephyr" if is_advanced_model else "basic")
+            formatted_prompt = format_enhanced_prompt(message, model_type="zephyr" if is_advanced_model else "basic", context={"is_greeting": any(word in message.lower() for word in ["hello", "hi", "hey", "greetings"])})
             print(f"[AI DEBUG] Using enhanced prompt formatter")
         except ImportError as e:
             # Fall back to basic formatting
@@ -3489,12 +3489,12 @@
             # Conservative parameters for smaller models
             generation_output = direct_huggingface_model.generate(
                 **input_ids,
-                max_new_tokens=60,  # Even shorter for better coherence
+                max_new_tokens=40,  # Significantly shorter for better coherence on simple models
                 do_sample=True,
-                temperature=0.2,  # Lower for consistency
-                top_k=30,
-                top_p=0.85,
-                repetition_penalty=1.8  # Add strong repetition penalty to prevent loops
+                temperature=0.1,  # Much lower for improved consistency
+                top_k=20,
+                top_p=0.80,
+                repetition_penalty=2.0  # Stronger repetition penalty to prevent nonsensical loops
             )
         
         # Decode the generated tokens to text
@@ -3521,17 +3521,24 @@
         else:
             # Enhanced response extraction for simpler models
             if "Assistant: I'll answer this question directly and clearly." in generated_text:
-                # Extract everything after our special prompt
-                response = generated_text.split("Assistant: I'll answer this question directly and clearly.")[1].strip()
+                # Extract only what comes after our explicit marker
+                extracted = generated_text.split("Assistant: I'll answer this question directly and clearly.")[1].strip()
+                # Limit to first 2-3 sentences for simple queries to avoid rambling
+                sentences = re.split(r'[.!?]\s+', extracted)
+                if len(message.split()) < 10 and len(sentences) > 3:
+                    response = ". ".join(sentences[:3]) + "."
+                else:
+                    response = extracted
             elif "Assistant:" in generated_text:
-                response = generated_text.split("Assistant:")[1].strip()
+                extracted = generated_text.split("Assistant:")[1].strip()
+                # Limit sentences for simple messages
+                sentences = re.split(r'[.!?]\s+', extracted)
+                if len(message.split()) < 10 and len(sentences) > 3:
+                    response = ". ".join(sentences[:3]) + "."
+                else:
+                    response = extracted
             else:
-                # Remove the prompt first
-                response = generated_text.replace(formatted_prompt, "").strip()
-                
-            # Clean up any remaining garbage text
-            response = re.sub(r'User:.*$', '', response, flags=re.DOTALL).strip()  # Remove any 'User:' and everything after
+                response = generated_text.replace(formatted_prompt, "").strip()[:150]  # Hard limit length
             response = re.sub(r'^I\'ll answer this question directly and clearly\.\s*', '', response).strip()  # Remove our special prompt if it's at the beginning
         
         # Clean and validate the response
@@ -3540,8 +3547,9 @@
         # Enhanced validation to detect low-quality or nonsensical responses
         is_invalid = False
         
+        # Stricter validation for simple queries
         # Check for empty, short or echo responses
-        if not response or response.strip() == "" or len(response) < 10 or message.lower() in response.lower():
+        if not response or response.strip() == "" or len(response) < 10 or message.lower() in response.lower() or "assistant" in response.lower()[0:20]:
             is_invalid = True
             print("[AI DEBUG] Response failed basic validation: empty, too short, or contains user message")
         
@@ -3559,7 +3567,7 @@
                     max_repeats = max(word_counts.values())
                     unique_ratio = len(word_counts) / len(words)
                     
-                    if max_repeats > 4 or unique_ratio < 0.4:  # Thresholds for repetition detection
+                    if max_repeats > 3 or unique_ratio < 0.5:  # Stricter thresholds for repetition detection
                         is_invalid = True
                         print(f"[AI DEBUG] Response failed repetition check: max_repeats={max_repeats}, unique_ratio={unique_ratio:.2f}")
         
@@ -3567,7 +3575,13 @@
         if not is_invalid:
             sentences = re.split(r'[.!?]\s+', response)
             if len(sentences) >= 3:  # Only analyze longer responses
-                sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
+                # Calculate average sentence length - abnormal length patterns indicate model confusion
+                sentence_lengths = [len(s.split()) for s in sentences if s.strip()]
+                avg_length = sum(sentence_lengths) / max(1, len(sentence_lengths))
+                
+                # For simple queries, check for first sentence being too long
+                if len(message.split()) < 7 and len(sentences) > 0 and len(sentences[0].split()) > 20:
+                    is_invalid = True
                 if sentence_lengths and (max(sentence_lengths) > 25 or min(sentence_lengths) < 2):
                     is_invalid = True
                     print(f"[AI DEBUG] Response failed sentence structure check: lengths={sentence_lengths}")
