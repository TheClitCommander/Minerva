"""
Multi-Model Processor for Minerva AI

This module integrates multiple AI models (HuggingFace, GPT4All, AutoGPT) 
and selects the best response based on quality metrics.
"""

import os
import sys
import re
import time
import asyncio
import threading
from typing import Dict, List, Optional, Any, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("multi_model_processor")

# Export simulated model processors at module level for use by MultiAICoordinator
simulated_gpt4_processor = None  # Will be defined later
simulated_claude3_processor = None  # Will be defined later
simulated_mistral7b_processor = None  # Will be defined later
simulated_gpt4all_processor = None  # Will be defined later

# Quality evaluation functions
def evaluate_response_quality(response: str, message: str, query_complexity: float = None, model_capabilities: Dict[str, Any] = None) -> float:
    """
    Evaluate the quality of a response based on various metrics including query complexity.
    
    Args:
        response: The AI-generated response
        message: The original user message
        query_complexity: Optional complexity score of the query (1-10)
        model_capabilities: Optional dict containing model specializations and capabilities
    
    Returns:
        Float score between 0-10, with higher being better
    """
    if not response or not isinstance(response, str):
        return 0.0
    
    # Determine query complexity if not provided
    if query_complexity is None:
        # Simple complexity heuristic based on message length and technical terms
        message_length = len(message)
        has_technical_terms = bool(re.search(r'(algorithm|function|code|implement|technical|optimize|parameter|variable|async|data structure)', message.lower()))
        query_complexity = min(10.0, (message_length / 100) + (4.0 if has_technical_terms else 0.0))
    
    score = 0.0
    
    # Length-based scoring - adjusted by complexity expectations
    length = len(response)
    expected_min_length = 30 * query_complexity  # More complex queries deserve longer responses
    expected_max_length = 500 + (200 * query_complexity)  # Scale with complexity
    
    if expected_min_length <= length <= expected_max_length:
        score += min(3.0, length / (100 + (25 * query_complexity)))  # Scaled ideal length scoring
    elif length > expected_max_length:
        # Less penalty for long responses to complex queries
        penalty_factor = 0.5 if query_complexity > 7 else 1.0
        score += 3.0 - min(3.0, ((length - expected_max_length) / 1000) * penalty_factor)
    else:
        score += length / expected_min_length * 1.5  # Adjusted for complexity
    
    # Coherence-based scoring (check for repetition)
    sentences = re.split(r'[.!?]', response)
    unique_sentences = set(s.strip().lower() for s in sentences if s.strip())
    repetition_ratio = len(unique_sentences) / max(1, len(sentences))
    score += 2.0 * repetition_ratio  # Up to 2 points for non-repetitive content
    
    # Detail and structure scoring - expectations increase with complexity
    structure_score = 0.0
    has_paragraphs = "\n\n" in response or "\n" in response
    structure_score += 0.5 if has_paragraphs else 0.0
    
    # Check for various structural elements that indicate organization
    has_sections = bool(re.search(r'\b(section|part|step|\d\.\s|\*\s|\-\s)', response))
    structure_score += 0.5 if has_sections else 0.0
    
    # For complex queries, we expect better structure
    if query_complexity > 5:
        expected_structure_score = 1.0
    else:
        expected_structure_score = 0.5
    
    score += min(1.0, structure_score * (1.0 + (0.5 * (query_complexity / 10))))
    
    # Detail scoring - more complexity should have more specific details
    contains_specifics = re.search(r'\d+|specific|example|instance|case|step', response.lower())
    detail_score = 1.0 if contains_specifics else 0.0
    # For highly complex queries, check for more advanced details
    if query_complexity > 7:
        contains_advanced_details = re.search(r'(algorithm|implementation|method|technique|approach|formula|equation)', response.lower())
        detail_score = detail_score * 1.5 if contains_advanced_details else detail_score
    
    score += min(1.5, detail_score)
    
    # Reasoning quality - more important for complex queries
    contains_reasoning = re.search(r'because|therefore|since|reason|explain|due to', response.lower())
    reasoning_score = 1.0 if contains_reasoning else 0.0
    
    # For complex queries, check for deeper reasoning patterns
    if query_complexity > 6:
        contains_deep_reasoning = re.search(r'(analytical|theoretical|conceptual|framework|underlying|fundamental|principle)', response.lower())
        reasoning_score = reasoning_score * 1.3 if contains_deep_reasoning else reasoning_score
    
    score += min(1.5, reasoning_score)
    
    # Politeness and helpfulness
    is_polite = not re.search(r'(^|\s)(stupid|idiot|dumb|shut up|ignorant)(\s|$)', response.lower())
    score += 1.0 if is_polite else 0.0  # 1 point for being polite
    
    # Relevance to the original message - weighted by complexity
    message_keywords = set(re.findall(r'\b\w+\b', message.lower()))
    response_keywords = set(re.findall(r'\b\w+\b', response.lower()))
    common_keywords = message_keywords & response_keywords
    
    # Calculate basic relevance score
    relevance_score = len(common_keywords) / max(1, len(message_keywords))
    
    # For complex queries, we also want to see related terms, not just the same keywords
    # This is a simple approximation - in a real system we might use embeddings
    if query_complexity > 5:
        technical_expansion = bool(re.search(r'(concept|theory|method|system|model|framework)', response.lower()))
        relevance_score = relevance_score * 1.2 if technical_expansion and len(common_keywords) > 2 else relevance_score
    
    score += min(2.5, 2.0 * relevance_score)  # Up to 2.5 points for relevance on complex queries
    
    # Model-specific capability scoring
    if model_capabilities:
        # If we know this model has strengths that match this query type
        if model_capabilities.get('technical_expertise') and query_complexity > 7:
            score += 0.5  # Bonus for technical models on technical queries
        if model_capabilities.get('creative') and 'creative' in message.lower():
            score += 0.5  # Bonus for creative models on creative queries
    
    # Trim the score to max 10
    return min(10.0, score)

async def process_with_model(model_name: str, message: str, processor_func, **kwargs) -> Dict[str, Any]:
    """
    Process a message with a specific model function.
    
    Args:
        model_name: Name of the model
        message: User message to process
        processor_func: Function to call for processing
        kwargs: Additional arguments for the processor
        
    Returns:
        Dict with model_name, response, quality_score, and processing_time
    """
    start_time = time.time()
    response = ""
    error = None
    
    # Ensure proper generation parameters based on model type
    model_type_kwargs = kwargs.copy()
    
    # Set default parameters if not provided
    if 'do_sample' not in model_type_kwargs:
        model_type_kwargs['do_sample'] = True
    
    # Adjust temperature based on model type
    if model_name.lower() == "huggingface":
        # Check if it's a more advanced model
        model_name_lower = kwargs.get('model_name', '').lower() if kwargs.get('model_name') else ''
        is_advanced = any(name in model_name_lower for name in ['zephyr', 'mistral', 'llama'])
        
        if 'temperature' not in model_type_kwargs:
            model_type_kwargs['temperature'] = 0.3 if is_advanced else 0.2
        
        # If temperature is set, always ensure do_sample is True
        if model_type_kwargs.get('temperature', 0) > 0:
            model_type_kwargs['do_sample'] = True
        
        # Add model-specific parameters
        if is_advanced and 'top_p' not in model_type_kwargs:
            model_type_kwargs['top_p'] = 0.9
        
        # Set appropriate model type for prompt formatting
        if 'model_type' not in model_type_kwargs:
            if 'zephyr' in model_name_lower:
                model_type_kwargs['model_type'] = 'zephyr'
            elif 'mistral' in model_name_lower:
                model_type_kwargs['model_type'] = 'mistral'
            else:
                model_type_kwargs['model_type'] = 'basic'
    
    try:
        # Call the processor function with the message and updated kwargs
        logger.info(f"Processing with {model_name} (do_sample={model_type_kwargs.get('do_sample')}, temp={model_type_kwargs.get('temperature', 'default')})...")
        response = await asyncio.to_thread(processor_func, message, **model_type_kwargs)
        
        # Calculate quality score
        quality_score = evaluate_response_quality(response, message)
        logger.info(f"{model_name} generated response with quality score: {quality_score:.2f}")
    except Exception as e:
        error = str(e)
        quality_score = 0.0
        logger.error(f"Error with {model_name}: {error}")
    
    processing_time = time.time() - start_time
    
    return {
        "model_name": model_name,
        "response": response,
        "quality_score": quality_score,
        "processing_time": processing_time,
        "error": error
    }

async def get_best_response(message: str, 
                            huggingface_func=None, 
                            gpt4all_func=None, 
                            autogpt_func=None,
                            timeout: float = 15.0) -> Dict[str, Any]:
    """
    Process a message with multiple models and return the best response.
    
    Args:
        message: User message to process
        huggingface_func: Function to call for HuggingFace processing
        gpt4all_func: Function to call for GPT4All processing
        autogpt_func: Function to call for AutoGPT processing
        timeout: Maximum time to wait for responses (seconds)
        
    Returns:
        Dict with best_model, response, quality_score, and all_responses
    """
    print(f"[AI DEBUG] Getting multi-model responses for: {message[:50]}{'...' if len(message) > 50 else ''}")
    
    response_tasks = {}
    response_results = {}
    
    # Create tasks for each available model
    if huggingface_func:
        response_tasks["huggingface"] = process_with_model("HuggingFace", message, huggingface_func)
        print("[AI DEBUG] Added HuggingFace model to response candidates")
    
    if gpt4all_func:
        response_tasks["gpt4all"] = process_with_model("GPT4All", message, gpt4all_func)
        print("[AI DEBUG] Added GPT4All model to response candidates")
    
    if autogpt_func:
        response_tasks["autogpt"] = process_with_model("AutoGPT", message, autogpt_func)
        print("[AI DEBUG] Added AutoGPT model to response candidates")
    
    # If no models are available, return a default response
    if not response_tasks:
        print("[ERROR] No AI models are available for multi-model processing")
        return {
            "response": "I'm sorry, I don't have access to AI models at the moment. Could you ask a different question?",
            "best_model": "none",
            "quality_score": 0.0
        }
    
    # Process model responses with timeout
    available_models = list(response_tasks.keys())
    print(f"[AI DEBUG] Waiting for responses from {len(available_models)} models: {', '.join(available_models)}")
    
    start_time = time.time()
    
    # Process each model's response
    for model_name, task in response_tasks.items():
        try:
            # If it's already a string (not a coroutine), use it directly
            if isinstance(task, str):
                response = task
            else:
                # Otherwise assume it's a coroutine, await it
                response = await asyncio.wait_for(task, timeout=timeout)
            
            # Make sure we got a valid string response
            if not isinstance(response, str):
                print(f"[WARNING] {model_name.upper()} returned non-string response: {type(response)}")
                continue
                
            if not response or len(response.strip()) < 10:
                print(f"[WARNING] {model_name.upper()} returned empty or very short response")
                continue
            
            # Store valid response
            response_results[model_name] = response
            print(f"[AI DEBUG] Received {len(response)} character response from {model_name.upper()}")
            print(f"[AI DEBUG] {model_name.upper()} sample: {response[:50]}...")
            
        except Exception as e:
            print(f"[ERROR] {model_name.upper()} processing error: {str(e)}")
    
    elapsed_time = time.time() - start_time
    print(f"[AI DEBUG] Collected responses in {elapsed_time:.2f} seconds from {len(response_results)}/{len(response_tasks)} models")
    
    # If no valid responses, return a default response
    if not response_results:
        print("[ERROR] No valid responses received from any model")
        return {
            "response": "I'm sorry, I wasn't able to generate a good response. Could you try a different question?",
            "best_model": "none",
            "quality_score": 0.0
        }
    
    # Evaluate and select the best response
    best_model = None
    best_score = -1
    best_response = None
    
    print("[AI DEBUG] Evaluating response quality from each model...")
    
    for model_name, response in response_results.items():
        # Evaluate response quality
        quality_score = evaluate_response_quality(response, message)
        print(f"[AI DEBUG] {model_name.upper()} quality score: {quality_score:.2f}")
        
        # Update best response if this one is better
        if quality_score > best_score:
            best_score = quality_score
            best_model = model_name
            best_response = response
    
    print(f"[AI DEBUG] Selected {best_model.upper()} as best model with score {best_score:.2f}")
    print(f"[AI DEBUG] Best response: {best_response[:100]}...")
    
    return {
        "response": best_response,
        "best_model": best_model,
        "quality_score": best_score
    }

def get_framework_manager():
    """Get or create a FrameworkManager instance."""
    try:
        from integrations.framework_manager import FrameworkManager
        return FrameworkManager()
    except ImportError:
        logger.warning("FrameworkManager not available")
        return None

def get_model_processors():
    """
    Get available model processing functions from framework integrations.
    In test mode, returns mock implementations.
    
    Returns:
        Dict with model processing functions
    """
    # In test mode, create mock implementations
    if os.environ.get("MINERVA_TEST_MODE") == "1":
        logger.info("Using mock model processors for testing")
        return {
            "openai": lambda msg: f"OpenAI mock response to: {msg}",
            "claude": lambda msg: f"Claude mock response to: {msg}",
            "huggingface": lambda msg: f"HuggingFace mock response to: {msg}"
        }
    
    # Real implementation for production
    processors = {
        "huggingface": None,
        "gpt4all": None, 
        "autogpt": None
    }
    
    # Initialize simulated processors for think tank testing
    # These functions generate realistic-looking but different responses 
    # to test the Think Tank comparison functionality
    
    def _simulated_gpt4_processor(message):
        """Simulated GPT-4 model for testing think tank functionality"""
        import time
        logger.info("[THINK TANK] Using simulated GPT-4 processor")
        time.sleep(0.7)  # Simulate processing time
        
        # Create a high-quality response for GPT-4
        lines = message.split('\n')
        topic = lines[0][:40] if lines else "your question"
        
        response = f"I'll address your question about {topic} comprehensively.\n\n"
        
        # Theory of relativity specific response
        if 'relativity' in message.lower():
            response += "The theory of relativity, developed by Albert Einstein, consists of two related theories: special relativity (1905) and general relativity (1915).\n\n"
            response += "Special relativity established that the laws of physics are the same for all non-accelerating observers and showed that the speed of light is constant regardless of the observer's motion. This led to the famous equation E=mc², which demonstrates the equivalence of mass and energy.\n\n"
            response += "General relativity extended these principles to accelerating reference frames and gravity. It describes gravity not as a force, but as a curvature of spacetime caused by mass and energy.\n\n"
            response += "For space travel, relativity has several implications:\n\n"
            response += "1. Time dilation: Time passes more slowly for objects moving at high speeds or in strong gravitational fields\n"
            response += "2. Space contraction: Objects appear contracted in the direction of motion at relativistic speeds\n"
            response += "3. Mass increase: Objects become more massive as they approach the speed of light\n"
            response += "4. Speed limit: The impossibility of exceeding the speed of light creates challenges for interstellar travel\n\n"
            response += "These effects have been confirmed by numerous experiments and are essential considerations in modern physics and astronomy."
        elif 'explain' in message.lower():
            response += f"To explain this thoroughly, I'll address several key aspects:\n\n"
            response += f"1. Historical context and development\n"
            response += f"2. Core principles and mechanisms\n"
            response += f"3. Mathematical formulations\n"
            response += f"4. Practical applications\n"
            response += f"5. Current scientific understanding\n\n"
            response += f"Let's explore each of these in detail...\n\n"
        
        if 'code' in message.lower() or 'programming' in message.lower():
            response += f"Here's a detailed code implementation:\n\n```python\n"
            response += f"class Solution:\n    def __init__(self, parameters):\n        self.params = parameters\n        self.results = {{}}\n\n"
            response += f"    def calculate(self, input_data):\n        # Comprehensive algorithm implementation\n        preprocessed = self._preprocess(input_data)\n        result = self._core_algorithm(preprocessed)\n        return self._postprocess(result)\n\n"
            response += f"    def _preprocess(self, data):\n        # Handle edge cases and normalize input\n        return data  # normalized data\n```\n\n"
        
        return response
    
    def _simulated_claude3_processor(message):
        """Simulated Claude-3 model for testing think tank functionality"""
        import time
        logger.info("[THINK TANK] Using simulated Claude-3 processor")
        time.sleep(0.6)  # Simulate processing time
        
        # Create a thoughtful, well-structured response for Claude-3
        lines = message.split('\n')
        topic = lines[0][:40] if lines else "your inquiry"
        
        response = f"Thank you for asking about {topic}. I'd be happy to help.\n\n"
        
        # Theory of relativity specific response
        if 'relativity' in message.lower():
            response += "## Einstein's Theory of Relativity\n\n"
            response += "Einstein's theory of relativity revolutionized our understanding of space, time, and gravity. It consists of two major components:\n\n"
            response += "### Special Relativity (1905)\n"
            response += "* Based on two postulates: the laws of physics are the same in all inertial reference frames, and the speed of light is constant for all observers\n"
            response += "* Introduced concepts of time dilation and length contraction\n"
            response += "* Unified space and time into a four-dimensional spacetime\n"
            response += "* Established the mass-energy equivalence through E=mc²\n\n"
            response += "### General Relativity (1915)\n"
            response += "* Extended relativity to non-inertial (accelerating) reference frames\n"
            response += "* Reinterpreted gravity as curvature of spacetime caused by mass and energy\n"
            response += "* Predicted phenomena like gravitational waves and black holes\n\n"
            response += "### Implications for Space Travel\n"
            response += "Relativity creates both challenges and opportunities for space exploration:\n\n"
            response += "* **Time Dilation:** Astronauts on high-speed journeys would experience less time passage than people on Earth\n"
            response += "* **Fuel Requirements:** The energy needed to approach the speed of light increases asymptotically\n"
            response += "* **Communication Delays:** Limited by light speed across vast distances\n"
            response += "* **Gravitational Assists:** Using spacetime curvature around massive objects to gain velocity\n\n"
            response += "Modern physics continues to build on Einstein's foundation, searching for a unified theory that reconciles relativity with quantum mechanics."
        elif 'explain' in message.lower():
            response += f"I'll analyze this topic from multiple perspectives:\n\n"
            response += f"### Historical Context\n"
            response += f"The development of this concept began with... [historical details]\n\n"
            response += f"### Foundational Principles\n"
            response += f"At its core, this subject is governed by several key principles...\n\n"
            response += f"### Practical Applications\n"
            response += f"This concept has transformed our world through applications in...\n\n"
        
        if 'code' in message.lower() or 'programming' in message.lower():
            response += f"Here's how we might implement this in code:\n\n```python\n"
            response += f"# A well-documented implementation\nimport numpy as np\n\ndef optimize_solution(parameters, constraints):\n    # Finds the optimal solution given parameters and constraints\n    # \n    # Args:\n    #     parameters: Dictionary of input parameters\n    #     constraints: List of constraint functions\n    # \n    # Returns:\n    #     Optimal solution object with methods for analysis\n    \n    # Implementation details\n    solution = minimize(objective_function, initial_guess, constraints)\n    return Solution(solution)\n```\n\n"
        
        return response
        
    def _simulated_mistral7b_processor(message):
        """Simulated Mistral-7B model for testing think tank functionality"""
        import time
        logger.info("[THINK TANK] Using simulated Mistral-7B processor")
        time.sleep(0.4)  # Simulate processing time
        
        # Create a concise but informative response for Mistral-7B
        lines = message.split('\n')
        topic = lines[0][:40] if lines else "your question"
        
        response = f"Here's what you should know about {topic}:\n\n"
        
        # Theory of relativity specific response
        if 'relativity' in message.lower():
            response += "Einstein's theory of relativity consists of special relativity (1905) and general relativity (1915).\n\n"
            response += "Special relativity shows that:\n"
            response += "- The speed of light (c) is constant for all observers\n"
            response += "- Time dilation occurs at high speeds\n"
            response += "- Energy and mass are related by E=mc²\n\n"
            response += "General relativity explains that:\n"
            response += "- Gravity is a curvature in spacetime caused by mass\n"
            response += "- Massive objects like stars bend light\n"
            response += "- Time runs slower in stronger gravitational fields\n\n"
            response += "For space travel, relativistic effects mean:\n"
            response += "- Approaching light speed requires infinite energy\n"
            response += "- Travelers age more slowly than those on Earth (time dilation)\n"
            response += "- Gravitational slingshots can be used for acceleration\n\n"
            response += "While warp drives remain theoretical, understanding relativity has enabled precise GPS systems and our modern understanding of the universe."
        elif 'explain' in message.lower():
            response += f"Key points about this topic:\n\n"
            response += f"1. Basic definition: [concise explanation]\n"
            response += f"2. Important components: [list of components]\n"
            response += f"3. How it works: [brief functional explanation]\n"
            response += f"4. Why it matters: [practical relevance]\n\n"
        
        if 'code' in message.lower() or 'programming' in message.lower():
            response += f"Simple implementation:\n\n```python\n"
            response += f"def solve_problem(data):\n    # Efficient algorithm\n    result = process(data)\n    return optimize(result)\n\n"
            response += f"# Example usage\noutput = solve_problem(input_data)\n```\n\n"
        
        return response
    
    def _simulated_gpt4all_processor(message):
        """Simulated GPT4All model for testing think tank functionality"""
        import time
        logger.info("[THINK TANK] Using simulated GPT4All processor")
        time.sleep(0.5)  # Simulate processing time
        
        # Create a reasonable looking response for the given message
        lines = message.split('\n')
        response = f"I'm GPT4All, helping with your query about {lines[0][:30]}...\n\n"
        
        # Theory of relativity specific response
        if 'relativity' in message.lower():
            response += "The theory of relativity was created by Albert Einstein. There are two parts: special relativity and general relativity.\n\n"
            response += "Special relativity says that:\n"
            response += "- Light always moves at the same speed\n"
            response += "- Time slows down when you move very fast\n"
            response += "- E=mc² means energy equals mass times the speed of light squared\n\n"
            response += "General relativity is about gravity:\n"
            response += "- Gravity is when space and time get bent by heavy objects\n"
            response += "- Planets move around the sun because space is curved\n\n"
            response += "For space travel, this means:\n"
            response += "- We can't go faster than light\n"
            response += "- Astronauts age slightly slower in space\n"
            response += "- Very heavy things like black holes can trap even light\n\n"
            response += "The book is available for purchase from Amazon.com."
        elif 'explain' in message.lower():
            response += f"To explain this topic thoroughly, I'll cover several key points:\n\n"
            response += f"1. First, let's understand the basic concepts\n"
            response += f"2. Then we'll explore the deeper implications\n"
            response += f"3. Finally, I'll summarize with practical applications\n\n"
        
        if 'code' in message.lower() or 'programming' in message.lower():
            response += f"Here's some relevant code that might help:\n\n```python\n"
            response += f"def process_data(input_data):\n    # Process the input\n    result = analyze(input_data)\n    return result\n```\n\n"
        
        # Add a conclusion
        response += f"I hope this helps answer your question about {lines[0][:20]}. Let me know if you need further clarification! The book is available for purchase from Amazon.com."
        
        return response
    
# Response validation functions
def validate_response(response: str, message: str, model_name: str = None) -> Tuple[bool, Dict[str, Any]]:
    """
    Validates a model response to determine if it meets minimum quality standards.
    
    Args:
        response: The model's response text to validate
        message: The original user message
        model_name: Optional name of the model that generated the response
        
    Returns:
        Tuple of (is_valid, details) where details contains validation metrics
    """
    # Initialize validation result details
    validation_result = {
        "is_valid": True,
        "model": model_name,
        "reasons": [],
        "metrics": {}
    }
    
    # Skip validation for very short messages (likely system messages)
    if len(message) < 3:
        return True, validation_result
    
    # Normalize text for comparison
    response_lower = response.lower()
    message_lower = message.lower()
    
    # Check 1: Detect completely empty or extremely short responses
    if not response or len(response.strip()) < 5:
        validation_result["is_valid"] = False
        validation_result["reasons"].append("Response too short or empty")
        return False, validation_result
        
    # Check 2: Detect excessive repetition
    repetition_score = check_repetition(response)
    validation_result["metrics"]["repetition_score"] = repetition_score
    if repetition_score > 0.4:  # 40% repetition threshold
        validation_result["is_valid"] = False
        validation_result["reasons"].append(f"Excessive repetition detected (score: {repetition_score:.2f})")
    
    # Check 3: Detect AI self-references
    if any(phrase in response_lower for phrase in [
        "as an ai", "as an artificial intelligence", "as a language model", 
        "as an assistant", "i'm an ai", "i am an ai", "i'm a language model",
        "i am a language model", "i'm not able to", "i don't have the ability to",
        "i cannot browse", "i don't have access to", "i cannot access"
    ]):
        validation_result["is_valid"] = False
        validation_result["reasons"].append("Response contains AI self-references")
    
    # Check 4: Detect nonsensical content based on common patterns
    nonsense_patterns = [
        r"\b(lorem ipsum|dolor sit amet)\b",
        r"\b(\w)\1{5,}\b",  # Words with 6+ repeated characters like "aaaaaaa"
        r"\b\d{10,}\b",  # Very long number sequences (10+ digits)
        r"\b[A-Za-z0-9]{20,}\b"  # Very long strings without spaces (likely garbage)
    ]
    
    for pattern in nonsense_patterns:
        if re.search(pattern, response_lower):
            validation_result["is_valid"] = False
            validation_result["reasons"].append("Response contains nonsensical content")
            break
    
    # Check 5: Detect generic templates and filler content
    generic_templates = [
        "I'd be happy to help you with that",
        "I'll try my best to assist you",
        "here's some information about",
        "I hope this helps!",
        "Let me know if you need anything else",
        "Feel free to ask if you have any other questions"
    ]
    
    # Count how many generic phrases are present
    template_count = sum(1 for template in generic_templates if template.lower() in response_lower)
    validation_result["metrics"]["template_phrases"] = template_count
    
    # Check for excessive generic phrases (adjust threshold based on response length)
    if template_count >= 3 or (template_count >= 2 and len(response) < 200):
        validation_result["is_valid"] = False
        validation_result["reasons"].append(f"Response contains too many generic template phrases ({template_count})")
    
    # Check 6: Special case - greeting messages should be concise and relevant
    greeting_words = ["hello", "hi", "hey", "greetings", "good morning", "good afternoon", "good evening"]
    is_greeting = any(word in message_lower for word in greeting_words) and len(message_lower.split()) <= 3
    
    if is_greeting:
        # For greetings, check for irrelevant content
        irrelevant_content = [
            "news", "article", "report", "latest", "update", "research", "study", "analysis",
            "published", "journal", "paper", "according to", "scientists", "experts"
        ]
        
        if any(term in response_lower for term in irrelevant_content):
            validation_result["is_valid"] = False
            validation_result["reasons"].append("Response to greeting contains irrelevant content")
        
        # Greeting responses should be concise
        if len(response) > 300:
            validation_result["is_valid"] = False
            validation_result["reasons"].append("Greeting response is too verbose")
    
    # Check 7: Calculate relevance score and decide based on threshold
    relevance_score = calculate_relevance_score(response, message)
    validation_result["metrics"]["relevance_score"] = relevance_score
    
    # Adjust threshold based on message complexity
    words = message.split()
    complexity_threshold = 0.3 if len(words) > 8 else 0.4
    
    # Simple greetings require higher relevance
    if is_greeting:
        complexity_threshold = 0.7
    
    if relevance_score < complexity_threshold:
        validation_result["is_valid"] = False
        validation_result["reasons"].append(f"Low relevance score: {relevance_score:.2f}")
    
    # Log validation results
    if model_name:
        if validation_result["is_valid"]:
            logger.info(f"[VALIDATION] Response from {model_name} passed validation with metrics: {validation_result['metrics']}")
        else:
            logger.warning(f"[VALIDATION] Response from {model_name} failed validation: {validation_result['reasons']}")
    
    return validation_result["is_valid"], validation_result

def check_repetition(text: str) -> float:
    """
    Checks for repetition in text by counting repeated sentences and phrases.
    Returns a score between 0 and 1, where higher values indicate more repetition.
    """
    # Normalize text
    text = text.strip().lower()
    
    # No repetition in very short texts
    if len(text) < 20:
        return 0.0
    
    # Split into sentences
    sentences = [s.strip() for s in re.split(r'[.!?]\s+', text) if s.strip()]
    
    # Count duplicate sentences
    total_sentences = len(sentences)
    unique_sentences = len(set(sentences))
    
    if total_sentences == 0:
        return 0.0
    
    # Calculate sentence repetition score
    sentence_repetition = 1 - (unique_sentences / total_sentences)
    
    # Extract 5-word phrases (sliding window)
    phrases = []
    words = text.split()
    for i in range(len(words) - 4):
        phrases.append(" ".join(words[i:i+5]))
    
    # Count duplicate phrases
    total_phrases = len(phrases)
    unique_phrases = len(set(phrases))
    
    if total_phrases == 0:
        return sentence_repetition  # Fall back to sentence-level repetition
    
    # Calculate phrase repetition score
    phrase_repetition = 1 - (unique_phrases / total_phrases)
    
    # Average the two scores (weigh sentence repetition more)
    return (sentence_repetition * 0.7) + (phrase_repetition * 0.3)

def calculate_relevance_score(response: str, query: str) -> float:
    """
    Calculates how relevant a response is to the original query.
    Returns a score between 0 and 1, where higher values indicate more relevance.
    """
    # Normalize text
    response = response.lower()
    query = query.lower()
    
    # Handle very short queries differently (like greetings)
    query_words = query.split()
    greeting_words = ["hello", "hi", "hey", "greetings", "good morning", "good afternoon", "good evening"]
    
    is_greeting = any(word in query_words for word in greeting_words) and len(query_words) <= 3
    
    if is_greeting:
        # For greetings, check if response is also a greeting and not too verbose
        response_has_greeting = any(word in response.split()[:5] for word in greeting_words)
        response_is_concise = len(response) < 150
        
        # Check for irrelevant content indicators in greeting responses
        irrelevant_content = [
            "news", "article", "report", "latest", "update", "research", "study", "analysis",
            "published", "journal", "paper", "according to", "scientists", "experts"
        ]
        
        has_irrelevant_content = any(term in response for term in irrelevant_content)
        
        # Calculate greeting-specific score
        greeting_score = 0.0
        if response_has_greeting:
            greeting_score += 0.6
        if response_is_concise:
            greeting_score += 0.4
        if has_irrelevant_content:
            greeting_score -= 0.7
        
        return max(0.0, min(1.0, greeting_score))
    
    # For normal queries, extract meaningful keywords (skip common words)
    stopwords = set([
        "a", "an", "the", "and", "or", "but", "if", "because", "as", "what", 
        "which", "this", "that", "these", "those", "then", "just", "so", "than", "such",
        "when", "where", "why", "how", "all", "any", "both", "each", "few", "more", 
        "most", "other", "some", "such", "no", "nor", "not", "only", "own", "same", "too",
        "very", "can", "will", "should", "now", "to", "of", "for", "with", "about",
        "is", "are", "was", "were", "be", "been", "being", "have", "has", "had", "do",
        "does", "did", "doing", "could", "would", "should", "shall", "might", "must"
    ])
    
    # Extract meaningful keywords from query
    query_keywords = [word for word in query_words if word not in stopwords and len(word) > 2]
    
    # Handle queries with no meaningful keywords
    if not query_keywords:
        # Fall back to using all words if no meaningful keywords
        query_keywords = query_words
    
    # Count keyword occurrences in response
    keyword_count = sum(1 for keyword in query_keywords if keyword in response)
    
    # Calculate basic keyword match score
    keyword_score = keyword_count / len(query_keywords) if query_keywords else 0.0
    
    # Check if response repeats the query verbatim (potential echo issue)
    if query in response and len(query) > 15:
        keyword_score *= 0.5  # Penalize exact copying of long queries
    
    # Check for informational content indicators
    informational_patterns = [
        r"\b\d+\b",  # Contains numbers
        r"[A-Z][a-z]+\s[A-Z][a-z]+",  # Proper nouns
        r"\b(however|therefore|furthermore|moreover|consequently)\b",  # Connecting words
        r"[:\---]\s*[a-z]",  # List markers
        r"\([^)]+\)",  # Parenthetical information
        r"""[^"""]+""",  # Quoted text
    ]
    
    # Bonus for having informational content
    information_score = min(0.3, sum(0.1 for pattern in informational_patterns if re.search(pattern, response)))
    
    # Combine scores with weights
    final_score = (keyword_score * 0.7) + information_score
    
    # Ensure score is between 0 and 1
    return max(0.0, min(1.0, final_score))

# Define direct simulated processors for easier access
def simulated_gpt4_processor(message):
    """GPT-4 simulated processor that returns a formatted, coherent response"""
    logger.info(f"[THINK TANK] simulated_gpt4_processor called with message: {message[:30]}...")
    return f"Simulated GPT-4 response to your question about {message.split()[0:3]}. This model would provide a detailed, nuanced explanation with clear examples and thorough analysis."

def simulated_claude3_processor(message):
    """Claude-3 simulated processor that returns a formatted, coherent response"""
    logger.info(f"[THINK TANK] simulated_claude3_processor called with message: {message[:30]}...")
    return f"Simulated Claude-3 response regarding {message.split()[0:3]}. This model tends to provide comprehensive, well-structured explanations with a focus on clarity and ethical considerations."

def simulated_mistral7b_processor(message):
    """Mistral 7B simulated processor that returns a formatted, coherent response"""
    logger.info(f"[THINK TANK] simulated_mistral7b_processor called with message: {message[:30]}...")
    return f"Simulated Mistral-7B responding to {message.split()[0:3]}. This model gives concise, informative responses that are technically accurate but less verbose than larger models."

def simulated_gpt4all_processor(message):
    """GPT4All simulated processor that returns a formatted, coherent response"""
    logger.info(f"[THINK TANK] simulated_gpt4all_processor called with message: {message[:30]}...")
    return f"Simulated GPT4All analysis of {message.split()[0:3]}. This model provides accessible explanations suitable for general audiences with practical examples."

# Make simulated processors available for import
__all__ = [
    "simulated_gpt4_processor",
    "simulated_claude3_processor",
    "simulated_mistral7b_processor",
    "simulated_gpt4all_processor"
]

logger.info("[THINK TANK] Simulated processors defined directly at module level")

# First, check for direct HuggingFace model availability
try:
    # Try to access the global variables from app.py
    from web.app import direct_huggingface_model, direct_huggingface_tokenizer, direct_huggingface_available, process_huggingface_only
    
    if direct_huggingface_available and direct_huggingface_model is not None:
        logger.info("Direct HuggingFace model is available, using it as processor")
        processors["huggingface"] = process_huggingface_only
except ImportError as e:
    logger.warning(f"Could not import direct HuggingFace model: {e}")
except Exception as e:
    logger.warning(f"Error accessing direct HuggingFace model: {e}")

def get_framework_manager():
    """Get the framework manager to access integrations"""
    try:
        # Avoid circular imports by importing here
        from web.integrations.framework_manager import get_framework_manager as get_manager
        return get_manager()
    except Exception as e:
        logger.warning(f"Error getting framework manager: {e}")
        return None

def format_enhanced_prompt(message: str, model_type: str = "basic") -> str:
    
    # Log available processors
    available = [name for name, proc in processors.items() if proc is not None]
    logger.info(f"Available model processors: {available}")
    
    return processors

def format_enhanced_prompt(message: str, model_type: str = "basic") -> str:
    """
    Format a more structured prompt for better AI responses.
    
    Args:
        message: The user message to format
        model_type: The type of model - "zephyr", "mistral", "basic", "gpt4all", "autogpt"
        
    Returns:
        str: Properly formatted prompt for the given model type
    """
    message = message.strip()
    
    # Base system prompt for all models
    system_prompt = """You are Minerva, a highly intelligent AI assistant designed to provide helpful, accurate, and thoughtful responses.
Follow these guidelines:
1. Provide clear, concise, and accurate information
2. When explaining complex topics, break them down into understandable parts
3. If you're unsure about something, acknowledge the uncertainty
4. Be respectful and considerate in your responses
5. Provide reasoning and examples when appropriate
"""
    
    if model_type == "zephyr":
        return f"<|system|>\n{system_prompt}</s>\n{message}"
    elif model_type == "mistral":
        return f"{system_prompt}\n\n{message}"
    elif model_type == "basic":
        return f"{system_prompt}\n\nPlease respond to the following message:\n{message}"
    elif model_type == "gpt4all":
        return f"{system_prompt}\n\nUser: {message}\nMinerva:"
    elif model_type == "autogpt":
        return f"{system_prompt}\n\nInput: {message}\nResponse:"
    else:
        return f"{system_prompt}\n\n{message}"


def analyze_message_complexity(message: str) -> int:
    """
    Analyze the complexity of a user message based on various factors.
    
    Args:
        message: The user's input message
        
    Returns:
        int: Complexity score from 1-10 with 10 being most complex
    """
    # Start with a base complexity based on length
    length_score = min(len(message) / 100, 4)  # Up to 4 points for length
    
    # Check for complex sentence structures
    complex_phrases = [
        "however", "although", "nevertheless", "consequently",
        "therefore", "furthermore", "moreover", "in contrast",
        "alternatively", "specifically", "conversely", "hence", 
        "thus", "accordingly", "in conclusion"
    ]
    structure_score = min(sum(1 for phrase in complex_phrases if phrase in message.lower()) * 0.5, 2)
    
    # Check for specialized terminology or topics
    specialized_terms = [
        "algorithm", "optimization", "methodology", "implementation", 
        "theoretical", "derivative", "integral", "quantum", "synthesis",
        "analysis", "evaluation", "coefficient", "parameter", "hypothesis",
        "inference", "calibration", "interpolation", "regression", "dependent"
    ]
    specialized_score = min(sum(1 for term in specialized_terms if term in message.lower()) * 0.5, 2)
    
    # Check for questions requiring multi-step reasoning
    multi_step_indicators = [
        "how would", "what if", "explain why", "compare and contrast",
        "analyze the", "synthesize", "evaluate the", "determine if",
        "investigate", "calculate", "prove", "disprove", "justify"
    ]
    reasoning_score = min(sum(1 for indicator in multi_step_indicators 
                              if indicator in message.lower()) * 0.5, 2)
    
    # Calculate total complexity score (1-10 scale)
    total_score = length_score + structure_score + specialized_score + reasoning_score
    complexity = max(1, min(round(total_score), 10))  # Ensure it's between 1-10
    
    return complexity


def get_query_tags(message: str) -> list:
    """
    Extract relevant tags from a user message to classify the query.
    
    Args:
        message: The user message to analyze
        
    Returns:
        list: A list of tag strings that describe the query
    """
    message_lower = message.lower()
    tags = []
    
    # Technical/coding tags
    if any(term in message_lower for term in ["code", "function", "programming", "algorithm", 
                                             "python", "javascript", "java", "html", "css", 
                                             "api", "database", "sql", "server"]):
        tags.append("technical")
        tags.append("code")
    
    # Math/Science tags
    if any(term in message_lower for term in ["math", "equation", "formula", "calculate", 
                                             "physics", "chemistry", "biology", "scientific", 
                                             "computation", "integral", "derivative"]):
        tags.append("mathematical")
        tags.append("scientific")
    
    # Business/Finance tags
    if any(term in message_lower for term in ["business", "finance", "economic", "market", 
                                             "stock", "investment", "profit", "loss", 
                                             "revenue", "strategy", "management"]):
        tags.append("financial")
        tags.append("business")
    
    # Creative/Narrative tags
    if any(term in message_lower for term in ["story", "creative", "write", "novel", 
                                             "poem", "fiction", "narrative", "character", 
                                             "plot", "imagine", "create"]):
        tags.append("creative")
        tags.append("narrative")
    
    # Analytical/Research tags
    if any(term in message_lower for term in ["analyze", "research", "study", "examine", 
                                             "investigate", "compare", "contrast", 
                                             "evaluate", "assess", "critique"]):
        tags.append("analytical")
        tags.append("research")
    
    # Educational/Informational tags
    if any(term in message_lower for term in ["explain", "teach", "learn", "understand", 
                                             "concept", "definition", "information", 
                                             "knowledge", "education", "tutorial"]):
        tags.append("educational")
        tags.append("informational")
    
    # If no specific tags were identified, add a general tag
    if not tags:
        tags.append("general")
    
    # Add length-based tag
    if len(message) > 300:
        tags.append("long_query")
    elif len(message) < 50:
        tags.append("short_query")
    
    # Question type tag
    if "?" in message:
        tags.append("question")
        if message_lower.startswith("how"):
            tags.append("how_question")
        elif message_lower.startswith("why"):
            tags.append("why_question")
        elif message_lower.startswith("what"):
            tags.append("what_question")
        elif message_lower.startswith("when"):
            tags.append("when_question")
        elif message_lower.startswith("where"):
            tags.append("where_question")
        elif message_lower.startswith("who"):
            tags.append("who_question")
    else:
        tags.append("statement")
    
    return tags


def route_request(user_message: str, mode="think_tank") -> dict:
    """
    Determines the best AI models for a specific request based on content analysis.
    Enhanced to support the think tank approach with additional query metadata.
    
    Args:
        user_message: The user's input message to analyze
        mode: Operation mode, defaults to 'think_tank' for multi-model evaluation
        
    Returns:
        dict: A decision dictionary with model list and metadata
    """
    # FORCE MULTIPLE MODELS FOR THINK TANK TESTING
    logger.info(f"[THINK TANK] Force-enabling multiple models for testing")
    
    # List of models we'll simulate for testing purposes
    simulated_models = ["gpt4", "claude3", "mistral7b", "huggingface"]
    logger.info(f"[THINK TANK] Available models for testing: {simulated_models}")
    
    # For regular operation, get actual available processors
    available_processors = get_model_processors()
    # Add our simulated GPT4All processor to ensure multiple models
    if "gpt4all" not in available_processors and simulated_gpt4all_processor is not None:
        available_processors["gpt4all"] = simulated_gpt4all_processor
        logger.info(f"[THINK TANK] Added simulated gpt4all processor")
    
    # Get real available models
    real_available_models = [name for name, proc in available_processors.items() if proc is not None]
    logger.info(f"[THINK TANK] Actually available models: {real_available_models}")
    
    # If testing mode active or no models available, use simulated models
    if mode == "think_tank" or not real_available_models:
        # Override with simulated models for testing
        available_models = simulated_models if mode == "think_tank" else ["huggingface"]
        logger.info(f"[THINK TANK] Using models: {available_models}")
    else:
        available_models = real_available_models
        
    # If still no models are available, return a default
    if not available_models:
        logger.warning("No models available for routing")
        return {
            "models": ["huggingface"],  # Default fallback
            "query_type": "general",
            "complexity": 1,
            "query_tags": ["general"],
            "think_tank_mode": False  # Can't run think tank with no models
        }
    
    message_lower = user_message.lower()
    
    # Determine query type based on content analysis
    query_type = "general"  # Default
    
    # Check for mathematical queries
    if any(term in message_lower for term in ["calculate", "math", "equation", "solve", "formula", "computation"]):
        query_type = "math"
    # Check for code-related queries
    elif any(term in message_lower for term in ["code", "programming", "function", "algorithm", "python", "javascript", "java", "html", "css"]):
        query_type = "code"
    # Check for finance/business queries
    elif any(term in message_lower for term in ["finance", "economics", "money", "business", "investment", "stocks", "market"]):
        query_type = "finance"
    # Check for complex queries
    elif len(user_message) > 200 or any(term in message_lower for term in ["explain", "analyze", "compare", "contrast", "synthesize", "evaluate"]):
        query_type = "complex"
    
    # Get query complexity (1-10 scale)
    complexity = analyze_message_complexity(user_message)
    
    # Get detailed query tags
    query_tags = get_query_tags(user_message)
    
    logger.info(f"Detected query type: {query_type}, complexity: {complexity}")
    logger.info(f"Query tags: {query_tags}")
    
    # ===== THINK TANK APPROACH =====
    # Instead of returning only one model, we'll return ALL available models
    # This way, the multi_ai_coordinator can query all models and select the best response
    
    # Order the models based on the query type preference
    # Mapping of query types to preferred models
    model_preferences = {
        "math": ["huggingface", "autogpt", "gpt4all"],
        "code": ["autogpt", "huggingface", "gpt4all"],
        "finance": ["huggingface", "autogpt", "gpt4all"],
        "complex": ["huggingface", "autogpt", "gpt4all"],
        "general": ["huggingface", "gpt4all", "autogpt"]
    }
    
    # Get the preferred order for this query type
    preferred_order = model_preferences.get(query_type, ["huggingface", "gpt4all", "autogpt"])
    
    # Sort available models according to preference order
    def get_preference_index(model):
        try:
            return preferred_order.index(model)
        except ValueError:
            return len(preferred_order)  # Put any models not in the preference list at the end
    
    sorted_models = sorted(available_models, key=get_preference_index)
    
    # Determine if think tank mode should be used (for complex queries or multiple available models)
    think_tank_mode = complexity >= 4 or len(available_models) > 1
    
    logger.info(f"Using think tank approach: {think_tank_mode}")
    logger.info(f"Models will be queried in priority order: {sorted_models}")
    
    # Return decision dictionary with models and metadata
    return {
        "models": sorted_models,
        "query_type": query_type,
        "complexity": complexity,
        "query_tags": query_tags,
        "think_tank_mode": think_tank_mode
    }

# Simulated processors are now defined directly at module level
logger.info("Simulated processors ready for use in think tank mode")
